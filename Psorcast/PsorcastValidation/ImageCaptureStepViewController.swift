//
//  ImageCaptureStepViewController.swift
//  PsorcastValidation
//
//  Copyright Â© 2019 Sage Bionetworks. All rights reserved.
//
// Redistribution and use in source and binary forms, with or without modification,
// are permitted provided that the following conditions are met:
//
// 1.  Redistributions of source code must retain the above copyright notice, this
// list of conditions and the following disclaimer.
//
// 2.  Redistributions in binary form must reproduce the above copyright notice,
// this list of conditions and the following disclaimer in the documentation and/or
// other materials provided with the distribution.
//
// 3.  Neither the name of the copyright holder(s) nor the names of any contributors
// may be used to endorse or promote products derived from this software without
// specific prior written permission. No license is granted to the trademarks of
// the copyright holders even if such marks are included in this software.
//
// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
// AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
// ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE
// FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
// DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
// SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
// CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
// OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
//

import AVFoundation
import UIKit
import BridgeApp
import BridgeAppUI
import MetalKit
import MetalPerformanceShaders
import Vision

open class ImageCaptureStepObject: RSDUIStepObject, RSDStepViewControllerVendor {
    
    private enum CodingKeys: String, CodingKey, CaseIterable {
        case cameraDevice
    }
    
    /// front or back camera
    var cameraDevice: CameraDeviceWrapper?
    
    /// Default type is `.imageCapture`.
    open override class func defaultType() -> RSDStepType {
        return .imageCapture
    }
    
    /// Override to set the properties of the subclass.
    override open func copyInto(_ copy: RSDUIStepObject) {
        super.copyInto(copy)
        guard let subclassCopy = copy as? ImageCaptureStepObject else {
            assertionFailure("Superclass implementation of the `copy(with:)` protocol should return an instance of this class.")
            return
        }
        subclassCopy.cameraDevice = self.cameraDevice
    }
    
    /// Override the decoder per device type b/c the task may require a different set of permissions depending upon the device.
    open override func decode(from decoder: Decoder, for deviceType: RSDDeviceType?) throws {
        let container = try decoder.container(keyedBy: CodingKeys.self)
        
        if container.contains(.cameraDevice) {
            self.cameraDevice = try container.decode(CameraDeviceWrapper.self, forKey: .cameraDevice)
        }
        
        try super.decode(from: decoder, for: deviceType)
    }
    
    open class func examples() -> [[String : RSDJSONValue]] {
        let jsonA: [String : RSDJSONValue] = [
            "identifier"   : "imagecapture",
            "type"         : "imageCapture",
            "title"        : "Title"
        ]
        
        return [jsonA]
    }
    
    public func instantiateViewController(with parent: RSDPathComponent?) -> (UIViewController & RSDStepController)? {
        return ImageCaptureStepViewController(step: self, parent: parent)
    }
}

open class ImageCaptureStepViewController: RSDStepViewController, UIImagePickerControllerDelegate, UINavigationControllerDelegate, MTKViewDelegate, AVCaptureVideoDataOutputSampleBufferDelegate {
    
    // MARK: Session Management
    
    private enum SessionSetupResult {
        case success
        case notAuthorized
        case configurationFailed
    }
    
    private let session = AVCaptureSession()
    private var isSessionRunning = false
    
    // Hand pose tracking
    private let videoDataOutputQueue = DispatchQueue(
      label: "CameraFeedOutput",
      qos: .userInteractive)
    
    @available(iOS 14.0, *)
    private(set) lazy var handPoseRequest: VNDetectHumanHandPoseRequest = {
        let request = VNDetectHumanHandPoseRequest()
        request.maximumHandCount = 2
        return request
    }()
    
    var fingerTips: [CGPoint] = []
    
    // Communicate with the session and other session objects on this queue.
    private let sessionQueue = DispatchQueue(label: "camera session queue")
    
    private var setupResult: SessionSetupResult = .success
    
    @objc dynamic var videoDeviceInput: AVCaptureDeviceInput!
    
    @IBOutlet private weak var previewView: CameraPreviewView!
    
    private let photoOutput = AVCapturePhotoOutput()
    
    private var inProgressPhotoCaptureDelegates = [Int64: PhotoCaptureProcessor]()
    
    @IBOutlet weak var handOverlayView: UIView!
    
    @IBOutlet weak var captureButton: UIButton!
    
    @IBOutlet weak var cameraToggleButton: UIButton!
    
    @IBOutlet public var cameraContainerView: UIView?
    open var cameraView: UIView {
        if let cameraContainerViewUnwrapped = cameraContainerView {
            return cameraContainerViewUnwrapped
        }
        return self.view
    }
    
    private let picker = UIImagePickerController()
    private let processingQueue = DispatchQueue(label: "org.sagebase.ResearchSuite.camera.processing")
    
    private let videoDeviceDiscoverySession = AVCaptureDevice.DiscoverySession(deviceTypes: [.builtInWideAngleCamera, .builtInDualCamera, .builtInTrueDepthCamera],
                                                                               mediaType: .video, position: .unspecified)
    /// Metal image overylay classes
    public var device: MTLDevice!
    var queue: MTLCommandQueue!
    var sourceTextureSize = CGSize.zero
    var overlayImageSize = CGSize.zero
    var sourceTexture: MTLTexture!
    var clear2BlackPipelineState: MTLComputePipelineState!
    var metalImageError = false
    var mtkView: MTKView? = nil
    
    var captureStep: ImageCaptureStepObject? {
        return self.step as? ImageCaptureStepObject
    }
    
    open override func viewDidLoad() {
        super.viewDidLoad()
        
        // Disable the UI. Enable the UI later, if and only if the session starts running.
        self.captureButton.isEnabled = false
        
        // Set up the video preview view.
        previewView.session = session
        previewView.videoPreviewLayer.videoGravity = .resizeAspectFill
        /*
         Check the video authorization status. Video access is required.
         */
        switch AVCaptureDevice.authorizationStatus(for: .video) {
        case .authorized:
            // The user has previously granted access to the camera.
            break
            
        case .notDetermined:
            /*
             The user has not yet been presented with the option to grant
             video access. Suspend the session queue to delay session
             setup until the access request has completed.
             */
            sessionQueue.suspend()
            AVCaptureDevice.requestAccess(for: .video, completionHandler: { granted in
                if !granted {
                    self.setupResult = .notAuthorized
                }
                self.sessionQueue.resume()
            })
            
        default:
            // The user has previously denied access.
            setupResult = .notAuthorized
        }
        
        /*
         Setup the capture session.
         In general, it's not safe to mutate an AVCaptureSession or any of its
         inputs, outputs, or connections from multiple threads at the same time.
         
         Don't perform these tasks on the main queue because
         AVCaptureSession.startRunning() is a blocking call, which can
         take a long time. Dispatch session setup to the sessionQueue, so
         that the main queue isn't blocked, which keeps the UI responsive.
         */
        sessionQueue.async {
            self.configureSession()
        }
    }
    
    // MARK: View Controller Life Cycle
    
    override open func viewWillAppear(_ animated: Bool) {
        super.viewWillAppear(animated)
        
        sessionQueue.async {
            switch self.setupResult {
            case .success:
                // Only setup observers and start the session if setup succeeded.
                self.addObservers()
                self.session.startRunning()
                self.isSessionRunning = self.session.isRunning
                
            case .notAuthorized:
                DispatchQueue.main.async {
                    let changePrivacySetting = "Psorcast doesn't have permission to use the camera, please change privacy settings"
                    let message = NSLocalizedString(changePrivacySetting, comment: "Alert message when the user has denied access to the camera")
                    let alertController = UIAlertController(title: "Psorcast", message: message, preferredStyle: .alert)
                    
                    alertController.addAction(UIAlertAction(title: NSLocalizedString("OK", comment: "Alert OK button"),
                                                            style: .cancel,
                                                            handler: nil))
                    
                    alertController.addAction(UIAlertAction(title: NSLocalizedString("Settings", comment: "Alert button to open Settings"),
                                                            style: .`default`,
                                                            handler: { _ in
                                                                UIApplication.shared.open(URL(string: UIApplication.openSettingsURLString)!,
                                                                                          options: [:],
                                                                                          completionHandler: nil)
                    }))
                    
                    self.present(alertController, animated: true, completion: nil)
                }
                
            case .configurationFailed:
                DispatchQueue.main.async {
                    let alertMsg = "Alert message when something goes wrong during capture session configuration"
                    let message = NSLocalizedString("Unable to capture image", comment: alertMsg)
                    let alertController = UIAlertController(title: "Psorcast", message: message, preferredStyle: .alert)
                    
                    alertController.addAction(UIAlertAction(title: NSLocalizedString("OK", comment: "Alert OK button"),
                                                            style: .cancel,
                                                            handler: nil))
                    
                    self.present(alertController, animated: true, completion: nil)
                }
            }
        }
    }
    
    override open func viewWillDisappear(_ animated: Bool) {
        sessionQueue.async {
            if self.setupResult == .success {
                self.session.stopRunning()
                self.isSessionRunning = self.session.isRunning
                self.removeObservers()
            }
        }
        
        super.viewWillDisappear(animated)
    }
    
    func setupMetal(viewSize: CGSize) {
        
        // Metal is not available on simulator, needs > A7 chip
        #if TARGET_OS_SIMULATOR
            return
        #endif
        
        // Only init metal device once
        if device == nil {
            device = MTLCreateSystemDefaultDevice()!
            queue = device.makeCommandQueue()
        }
        
        var image: UIImage? = nil
        let stepId = self.stepViewModel.step.identifier
        
        #if !VALIDATION
            image = ImageDataManager.shared.getMostRecentHandFootImage(identifier: stepId)
        #endif
        
        if image == nil { // try the backup, default overlay
            image = UIImage(named: "CameraOverlay" + stepId.capitalizingFirstLetter())
        }
        
        guard let imageSize = image?.size else {
            self.metalImageError = true
            debugPrint("Error creating overlay image")
            return
        }
        
        let aspectFitRect = PSRImageHelper.calculateAspectFit(
            imageWidth: imageSize.width, imageHeight: imageSize.height,
            imageViewWidth: viewSize.width, imageViewHeight: viewSize.height)
        
        let textureSizeScaled = CGSize(width: aspectFitRect.width * UIScreen.main.scale,
                                       height: aspectFitRect.height * UIScreen.main.scale)
        
        if textureSizeScaled == self.sourceTextureSize {
            // Already scaled to this size
            return
        }
        
        let mtkViewStrong = MTKView(frame: aspectFitRect)
        mtkViewStrong.isPaused = true
        mtkViewStrong.enableSetNeedsDisplay = true
        mtkViewStrong.framebufferOnly = false
        mtkViewStrong.isOpaque = false
        mtkViewStrong.delegate = self
        mtkViewStrong.device = self.device
        self.mtkView?.removeFromSuperview()
        self.previewView.addSubview(mtkViewStrong)
        self.mtkView = mtkViewStrong
        
        guard let imageUnwraped = image?.resizeImage(targetSize: textureSizeScaled) else {
            self.metalImageError = true
            debugPrint("Error creating overlay image")
            return
        }
        
        // Create pipeline, expensive op, only do once
        if self.clear2BlackPipelineState == nil {
            self.clear2BlackPipelineState = self.device.createBlack2Clear()
        }

        self.sourceTexture = self.device.createMTLTexture(from: imageUnwraped)
        
        self.sourceTextureSize = textureSizeScaled
    }
    
    public func mtkView(_ view: MTKView, drawableSizeWillChange size: CGSize) {
        self.mtkView?.setNeedsDisplay()
    }
    
    public func draw(in view: MTKView) {
        
        #if TARGET_OS_SIMULATOR
            return
        #endif
        
        if self.metalImageError {
            return
        }
        
        guard let commandBuffer = queue.makeCommandBuffer(),
              let drawable = view.currentDrawable,
              let intermediateTexture = self.device.createEmptyMTLTexture(size: self.sourceTextureSize) else {
            debugPrint("Could not create textures to perform draw")
            return
        }

        // Encode the edge detection shader first in the command q
        let edgeDetectionShader = MPSImageSobel(device: device)
        edgeDetectionShader.encode(commandBuffer: commandBuffer, sourceTexture: sourceTexture,
                      destinationTexture: intermediateTexture)

        // Next we run a custom compute shader to transform black pixels to clear
        guard let commandEncoder = commandBuffer.makeComputeCommandEncoder() else {
            debugPrint("Could not create encoder to custom metal kernal func")
            return
        }
        commandEncoder.setComputePipelineState(self.clear2BlackPipelineState)
        commandEncoder.setTextures([drawable.texture, intermediateTexture], range: 0..<2)
        let threadGroupSize = MTLSizeMake(Int(sourceTextureSize.width),
                                          Int(sourceTextureSize.height), 1)
        commandEncoder.dispatchThreadgroups(threadGroupSize, threadsPerThreadgroup: MTLSizeMake(1, 1, 1))
        commandEncoder.endEncoding()

        commandBuffer.present(drawable)
        commandBuffer.commit()
    }
    
    private var keyValueObservations = [NSKeyValueObservation]()
    /// - Tag: ObserveInterruption
    private func addObservers() {
        let keyValueObservation = session.observe(\.isRunning, options: .new) { _, change in
            guard change.newValue != nil else { return }
            DispatchQueue.main.async {
                self.captureButton.isEnabled = true
            }
        }
        keyValueObservations.append(keyValueObservation)
        
        let systemPressureStateObservation = observe(\.videoDeviceInput.device.systemPressureState, options: .new) { _, change in
            guard let systemPressureState = change.newValue else { return }
            self.setRecommendedFrameRateRangeForPressureState(systemPressureState: systemPressureState)
        }
        keyValueObservations.append(systemPressureStateObservation)
        
        NotificationCenter.default.addObserver(self,
                                               selector: #selector(subjectAreaDidChange),
                                               name: .AVCaptureDeviceSubjectAreaDidChange,
                                               object: videoDeviceInput.device)
        
        NotificationCenter.default.addObserver(self,
                                               selector: #selector(sessionRuntimeError),
                                               name: .AVCaptureSessionRuntimeError,
                                               object: session)
        
        /*
         A session can only run when the app is full screen. It will be interrupted
         in a multi-app layout, introduced in iOS 9, see also the documentation of
         AVCaptureSessionInterruptionReason. Add observers to handle these session
         interruptions and show a preview is paused message. See the documentation
         of AVCaptureSessionWasInterruptedNotification for other interruption reasons.
         */
        NotificationCenter.default.addObserver(self,
                                               selector: #selector(sessionWasInterrupted),
                                               name: .AVCaptureSessionWasInterrupted,
                                               object: session)
        NotificationCenter.default.addObserver(self,
                                               selector: #selector(sessionInterruptionEnded),
                                               name: .AVCaptureSessionInterruptionEnded,
                                               object: session)
    }
    
    private func removeObservers() {
        NotificationCenter.default.removeObserver(self)
        
        for keyValueObservation in keyValueObservations {
            keyValueObservation.invalidate()
        }
        keyValueObservations.removeAll()
    }
    
    /// - Tag: HandleSystemPressure
    private func setRecommendedFrameRateRangeForPressureState(systemPressureState: AVCaptureDevice.SystemPressureState) {
        /*
         The frame rates used here are only for demonstration purposes.
         Your frame rate throttling may be different depending on your app's camera configuration.
         */
        let pressureLevel = systemPressureState.level
        if pressureLevel == .serious || pressureLevel == .critical {
            do {
                try self.videoDeviceInput.device.lockForConfiguration()
                print("WARNING: Reached elevated system pressure level: \(pressureLevel). Throttling frame rate.")
                self.videoDeviceInput.device.activeVideoMinFrameDuration = CMTime(value: 1, timescale: 20)
                self.videoDeviceInput.device.activeVideoMaxFrameDuration = CMTime(value: 1, timescale: 15)
                self.videoDeviceInput.device.unlockForConfiguration()
            } catch {
                print("Could not lock device for configuration: \(error)")
            }
        } else if pressureLevel == .shutdown {
            print("Session stopped running due to shutdown system pressure level.")
        }
    }
    
    /// - Tag: HandleInterruption
    @objc
    func sessionWasInterrupted(notification: NSNotification) {
        /*
         In some scenarios you want to enable the user to resume the session.
         For example, if music playback is initiated from Control Center while
         using Psorcast imaging, then the user can let Psorcast resume
         the session running, which will stop music playback. Note that stopping
         music playback in Control Center will not automatically resume the session.
         Also note that it's not always possible to resume, see `resumeInterruptedSession(_:)`.
         */
        if let userInfoValue = notification.userInfo?[AVCaptureSessionInterruptionReasonKey] as AnyObject?,
            let reasonIntegerValue = userInfoValue.integerValue,
            let reason = AVCaptureSession.InterruptionReason(rawValue: reasonIntegerValue) {
            print("Capture session was interrupted with reason \(reason)")
            
            if reason == .audioDeviceInUseByAnotherClient || reason == .videoDeviceInUseByAnotherClient {
            } else if reason == .videoDeviceNotAvailableWithMultipleForegroundApps {
                // TODO: mdephillips 1/22/20 Do we need to inform the user that the camera is unavailable?
                
            } else if reason == .videoDeviceNotAvailableDueToSystemPressure {
                print("Session stopped running due to shutdown system pressure level.")
            }
        }
    }
    
    @IBAction private func resumeInterruptedSession(_ resumeButton: UIButton) {
        sessionQueue.async {
            /*
             The session might fail to start running, for example, if a phone or FaceTime call is still
             using audio or video. This failure is communicated by the session posting a
             runtime error notification. To avoid repeatedly failing to start the session,
             only try to restart the session in the error handler if you aren't
             trying to resume the session.
             */
            self.session.startRunning()
            self.isSessionRunning = self.session.isRunning
            if !self.session.isRunning {
                DispatchQueue.main.async {
                    let message = NSLocalizedString("Unable to resume capture session", comment: "Alert message when unable to resume the session running")
                    let alertController = UIAlertController(title: "Psorcast", message: message, preferredStyle: .alert)
                    let cancelAction = UIAlertAction(title: NSLocalizedString("OK", comment: "Alert OK button"), style: .cancel, handler: nil)
                    alertController.addAction(cancelAction)
                    self.present(alertController, animated: true, completion: nil)
                }
            } else {
                DispatchQueue.main.async {
                    // No need to show resume button, just always resume
                    //self.resumeButton.isHidden = true
                }
            }
        }
    }
    
    @objc
    func sessionInterruptionEnded(notification: NSNotification) {
        print("Capture session interruption ended")
    }
    
    @objc
    func subjectAreaDidChange(notification: NSNotification) {
    }
    
    /// - Tag: HandleRuntimeError
    @objc
    func sessionRuntimeError(notification: NSNotification) {
        guard let error = notification.userInfo?[AVCaptureSessionErrorKey] as? AVError else { return }
        
        print("Capture session runtime error: \(error)")
//        // If media services were reset, and the last start succeeded, restart the session.
        if error.code == .mediaServicesWereReset {
            sessionQueue.async {
                if self.isSessionRunning {
                    self.session.startRunning()
                    self.isSessionRunning = self.session.isRunning
                } else {
                    DispatchQueue.main.async {
                        //self.resumeButton.isHidden = false
                    }
                }
            }
        } else {
            //resumeButton.isHidden = false
        }
    }
    
    override open var shouldAutorotate: Bool {
        return false
    }
    
    override open var supportedInterfaceOrientations: UIInterfaceOrientationMask {
        return .portrait
    }
    
    private func save(_ imageData: Data, to url: URL) {
        processingQueue.async {
            do {
                try imageData.write(to: url)
            } catch let error {
                debugPrint("Failed to save the camera image: \(error)")
            }
        }
    }
    
    open override func viewDidLayoutSubviews() {
        super.viewDidLayoutSubviews()
        
        self.captureButton.layer.cornerRadius = self.captureButton.bounds.width / 2
        
        // At this point, we know the width/height of MTKView
        // and we can setup metal so the texture matches
        self.setupMetal(viewSize: previewView.bounds.size)
    }
    
    /// Set the color style for the given placement elements. This allows overriding by subclasses to
    /// customize the view style.
    override open func setColorStyle(for placement: RSDColorPlacement, background: RSDColorTile) {
        
        super.setColorStyle(for: placement, background: background)
        
        if placement == .footer {
            self.captureButton.backgroundColor = self.designSystem.colorRules.palette.secondary.normal.color
            self.captureButton.tintColor = UIColor.white
            self.navigationFooter?.backgroundColor = UIColor.white
        } else if placement == .header {
            self.navigationHeader?.backgroundColor = UIColor.white
            self.navigationHeader?.titleLabel?.textColor = self.designSystem.colorRules.textColor(on: background, for: .largeHeader)
        }
    }
    
    // Call this on the session queue.
    /// - Tag: ConfigureSession
    private func configureSession() {
        if setupResult != .success {
            return
        }
        
        session.beginConfiguration()
        
        /*
         Do not create an AVCaptureMovieFileOutput when setting up the session because
         Live Photo is not supported when AVCaptureMovieFileOutput is added to the session.
         */
        session.sessionPreset = .photo
        
        // Add video input.
        do {
            var defaultVideoDevice: AVCaptureDevice?
            var cameraDevice = self.captureStep?.cameraDevice?.cameraDevice()
            
            // If camera device is both,
            // look to see if we have any cached settings
            if self.captureStep?.cameraDevice == .both,
                let cachedCameraString = UserDefaults.standard.string(forKey: "\(self.step.identifier)CameraSetting"),
                let cachedCameraDevice = CameraDeviceWrapper(rawValue: cachedCameraString)?.cameraDevice() {
                                
                cameraDevice = cachedCameraDevice
            }
            
            // Choose the back dual camera, if available, otherwise default to a wide angle camera.
            if cameraDevice == .front,
                let frontCameraDevice = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .front) {
                defaultVideoDevice = frontCameraDevice
            } else if let dualCameraDevice = AVCaptureDevice.default(.builtInDualCamera, for: .video, position: .back) {
                defaultVideoDevice = dualCameraDevice
            } else if let backCameraDevice = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .back) {
                // If a rear dual camera is not available, default to the rear wide angle camera.
                defaultVideoDevice = backCameraDevice
            }
            
            guard let videoDevice = defaultVideoDevice else {
                print("Default video device is unavailable.")
                setupResult = .configurationFailed
                session.commitConfiguration()
                return
            }
            let videoDeviceInput = try AVCaptureDeviceInput(device: videoDevice)
            
            if session.canAddInput(videoDeviceInput) {
                session.addInput(videoDeviceInput)
                self.videoDeviceInput = videoDeviceInput
                
                DispatchQueue.main.async {
                    // Lock the camera capture to portrait
                    self.previewView.videoPreviewLayer.connection?.videoOrientation = .portrait
                    
                    if self.captureStep?.cameraDevice == .both {
                        // Show the camera toggle button
                        self.cameraToggleButton?.isHidden = false
                        self.cameraToggleButton?.isEnabled = self.videoDeviceDiscoverySession.devices.count > 1
                    } else {
                        self.cameraToggleButton?.isHidden = true
                    }
                    
                    // Setup tap to focus functionality on overlay imageview
                    if self.cameraView.gestureRecognizers == nil {
                        self.cameraView.gestureRecognizers = []
                    }
                    self.cameraView.gestureRecognizers?.append(UITapGestureRecognizer(target: self, action: #selector(self.focusAndExposeTap)))
                }
            } else {
                print("Couldn't add video device input to the session.")
                setupResult = .configurationFailed
                session.commitConfiguration()
                return
            }
            
        } catch {
            print("Couldn't create video device input: \(error)")
            setupResult = .configurationFailed
            session.commitConfiguration()
            return
        }
        
        // Add the photo output
        if session.canAddOutput(photoOutput) {
            session.addOutput(photoOutput)
            photoOutput.isHighResolutionCaptureEnabled = true
            photoOutput.isLivePhotoCaptureEnabled = false
            photoOutput.isDepthDataDeliveryEnabled = false
            photoOutput.isPortraitEffectsMatteDeliveryEnabled = false
        } else {
            print("Could not add photo output to the session")
            setupResult = .configurationFailed
            session.commitConfiguration()
            return
        }
        
        // Hand pose tracking output
        if #available(iOS 14.0, *) {
            let dataOutput = AVCaptureVideoDataOutput()
            if session.canAddOutput(dataOutput) {
              session.addOutput(dataOutput)
              dataOutput.alwaysDiscardsLateVideoFrames = true
              dataOutput.setSampleBufferDelegate(self, queue: videoDataOutputQueue)
            } else {
                print("Could not add hand pose tracking to output to the session")
                setupResult = .configurationFailed
                session.commitConfiguration()
                return
            }
        }
        
        session.commitConfiguration()
    }
    
    /// - Tag: CapturePhoto
    @IBAction func capturePhoto(_ photoButton: UIButton) {
        
        let videoLayer = previewView.videoPreviewLayer
                
        // Retrieve the video preview layer's video orientation on the main queue before
        // entering the session queue. Do this to ensure that UI elements are accessed on
        // the main thread and session configuration is done on the session queue.
        let videoPreviewLayerOrientation = videoLayer.connection?.videoOrientation
        
        // The output rect of the video preview will be used to crop the photo to the preview size
        let outputRect = videoLayer.metadataOutputRectConverted(fromLayerRect: videoLayer.bounds)
        
        let flashMode: AVCaptureDevice.FlashMode = .on
        
        sessionQueue.async {
            if let photoOutputConnection = self.photoOutput.connection(with: .video) {
                photoOutputConnection.videoOrientation = videoPreviewLayerOrientation!
            }
            var photoSettings = AVCapturePhotoSettings()
            
            // Capture HEIF photos when supported. Enable auto-flash and high-resolution photos.
            if  self.photoOutput.availablePhotoCodecTypes.contains(.hevc) {
                photoSettings = AVCapturePhotoSettings(format: [AVVideoCodecKey: AVVideoCodecType.hevc])
            }
            
            if self.videoDeviceInput.device.isFlashAvailable {
                photoSettings.flashMode = flashMode
            }
            
            photoSettings.isHighResolutionPhotoEnabled = true
            if !photoSettings.__availablePreviewPhotoPixelFormatTypes.isEmpty {
                photoSettings.previewPhotoFormat = [kCVPixelBufferPixelFormatTypeKey as String: photoSettings.__availablePreviewPhotoPixelFormatTypes.first!]
            }
            
            photoSettings.isDepthDataDeliveryEnabled = false
            photoSettings.isPortraitEffectsMatteDeliveryEnabled = false
            
            let photoCaptureProcessor = PhotoCaptureProcessor(with: photoSettings, willCapturePhotoAnimation: {
                // Flash the screen to signal that Psorcast took a photo.
                DispatchQueue.main.async {
                    NSLog(" Flash the screen")
                    self.previewView.videoPreviewLayer.opacity = 0
                    UIView.animate(withDuration: 0.25) {
                        self.previewView.videoPreviewLayer.opacity = 1
                    }
                    self.captureButton.isEnabled = false
                }
            }, completionHandler: { photoCaptureProcessor in
                // When the capture is complete, remove a reference to the photo capture delegate so it can be deallocated.
                NSLog("reference to the photo capture delegate")
                self.sessionQueue.async {
                    let pngData = self.inProgressPhotoCaptureDelegates[photoCaptureProcessor.requestedPhotoSettings.uniqueID]?.photoData
                self.inProgressPhotoCaptureDelegates[photoCaptureProcessor.requestedPhotoSettings.uniqueID] = nil
                    self.session.stopRunning()
                    
                    DispatchQueue.main.async {
                        NSLog("reference to the photo capture delegate")
                        
                        if let photoPngData = pngData {
                            self.saveCapturedPhotoAndGoForward(pngData: photoPngData)
                        }
                    }
                }
            }, stopSessionRequestHandler: { photoCaptureProcessor in
                // When the capture is complete, immediately stop the session so the
                // image is on the screen
                NSLog("stopSessionRequestHandler")
                self.sessionQueue.async {
                    self.session.stopRunning()
                }
            }, photoProcessingHandler: { animate in
                // Animates a spinner while photo is processing
                DispatchQueue.main.async {
                    // TODO: mdephillips 12/20/19 need animation spinner?
                    NSLog("photo is processing")
                }
            })
                    
            // Setting this will make sure output image matches video preview
            photoCaptureProcessor.metaDataOutputRect = outputRect
            
            // The photo output holds a weak reference to the photo capture delegate and stores it in an array to maintain a strong reference.
            self.inProgressPhotoCaptureDelegates[photoCaptureProcessor.requestedPhotoSettings.uniqueID] = photoCaptureProcessor
            self.photoOutput.capturePhoto(with: photoSettings, delegate: photoCaptureProcessor)
            NSLog("finished session")
        }
    }
    
    /// - Tag: ChangeCamera
    @IBAction func changeCamera(_ cameraButton: UIButton) {
        self.cameraToggleButton?.isEnabled = false
        self.captureButton.isEnabled = false
        
        sessionQueue.async {
            let currentVideoDevice = self.videoDeviceInput.device
            let currentPosition = currentVideoDevice.position
            
            let preferredPosition: AVCaptureDevice.Position
            let preferredDeviceType: AVCaptureDevice.DeviceType
            
            switch currentPosition {
            case .unspecified, .front:
                preferredPosition = .back
                preferredDeviceType = .builtInDualCamera
            case .back:
                preferredPosition = .front
                preferredDeviceType = .builtInWideAngleCamera
            default:
                preferredPosition = .back
                preferredDeviceType = .builtInDualCamera
            }
            let devices = self.videoDeviceDiscoverySession.devices
            var newVideoDevice: AVCaptureDevice? = nil
            
            // Save the new camera position setting on the main thread
            // Next time this step runs under "both" cams it will default to the saved setting
            DispatchQueue.main.async {
                if preferredPosition == .front {
                    UserDefaults.standard.set(CameraDeviceWrapper.front.rawValue, forKey: "\(self.step.identifier)CameraSetting")
                } else if preferredPosition == .back {
                    UserDefaults.standard.set(CameraDeviceWrapper.rear.rawValue, forKey: "\(self.step.identifier)CameraSetting")
                }
            }
            
            // First, seek a device with both the preferred position and device type. Otherwise, seek a device with only the preferred position.
            if let device = devices.first(where: { $0.position == preferredPosition && $0.deviceType == preferredDeviceType }) {
                newVideoDevice = device
            } else if let device = devices.first(where: { $0.position == preferredPosition }) {
                newVideoDevice = device
            }
            
            if let videoDevice = newVideoDevice {
                do {
                    let videoDeviceInput = try AVCaptureDeviceInput(device: videoDevice)
                    
                    self.session.beginConfiguration()
                    
                    // Remove the existing device input first, because AVCaptureSession doesn't support
                    // simultaneous use of the rear and front cameras.
                    self.session.removeInput(self.videoDeviceInput)
                    
                    if self.session.canAddInput(videoDeviceInput) {
                        NotificationCenter.default.removeObserver(self, name: .AVCaptureDeviceSubjectAreaDidChange, object: currentVideoDevice)
                        NotificationCenter.default.addObserver(self, selector: #selector(self.subjectAreaDidChange), name: .AVCaptureDeviceSubjectAreaDidChange, object: videoDeviceInput.device)
                        
                        self.session.addInput(videoDeviceInput)
                        self.videoDeviceInput = videoDeviceInput
                    } else {
                        self.session.addInput(self.videoDeviceInput)
                    }
                    
                    /*
                     Set Live Photo capture and depth data delivery if it's supported. When changing cameras, the
                     `livePhotoCaptureEnabled` and `depthDataDeliveryEnabled` properties of the AVCapturePhotoOutput
                     get set to false when a video device is disconnected from the session. After the new video device is
                     added to the session, re-enable them on the AVCapturePhotoOutput, if supported.
                     */
                    self.photoOutput.isLivePhotoCaptureEnabled = self.photoOutput.isLivePhotoCaptureSupported
                    self.photoOutput.isDepthDataDeliveryEnabled = self.photoOutput.isDepthDataDeliverySupported
                    self.photoOutput.isPortraitEffectsMatteDeliveryEnabled = self.photoOutput.isPortraitEffectsMatteDeliverySupported

                    self.session.commitConfiguration()
                } catch {
                    print("Error occurred while creating video device input: \(error)")
                }
            }
            
            DispatchQueue.main.async {
                self.cameraToggleButton?.isEnabled = true
                self.captureButton.isEnabled = true
            }
        }
    }
    
    @objc func focusAndExposeTap(_ gestureRecognizer: UITapGestureRecognizer) {
        let devicePoint = previewView.videoPreviewLayer.captureDevicePointConverted(fromLayerPoint: gestureRecognizer.location(in: gestureRecognizer.view))
        focus(with: .autoFocus, exposureMode: .autoExpose, at: devicePoint, monitorSubjectAreaChange: true)
    }
    
    private func focus(with focusMode: AVCaptureDevice.FocusMode,
                       exposureMode: AVCaptureDevice.ExposureMode,
                       at devicePoint: CGPoint,
                       monitorSubjectAreaChange: Bool) {
        
        sessionQueue.async {
            let device = self.videoDeviceInput.device
            do {
                try device.lockForConfiguration()
                
                /*
                 Setting (focus/exposure)PointOfInterest alone does not initiate a (focus/exposure) operation.
                 Call set(Focus/Exposure)Mode() to apply the new point of interest.
                 */
                if device.isFocusPointOfInterestSupported && device.isFocusModeSupported(focusMode) {
                    device.focusPointOfInterest = devicePoint
                    device.focusMode = focusMode
                }
                
                if device.isExposurePointOfInterestSupported && device.isExposureModeSupported(exposureMode) {
                    device.exposurePointOfInterest = devicePoint
                    device.exposureMode = exposureMode
                }
                
                device.isSubjectAreaChangeMonitoringEnabled = monitorSubjectAreaChange
                device.unlockForConfiguration()
            } catch {
                print("Could not lock device for configuration: \(error)")
            }
        }
    }
    
    func saveCapturedPhotoAndGoForward(pngData: Data?) {
        guard let pngDataUnwrapped = pngData else {
            return
        }
        
        var url: URL?
        do {
            if let jpegData = PSRImageHelper.convertToJpegData(pngData: pngDataUnwrapped),
                let outputDir = self.stepViewModel.parentTaskPath?.outputDirectory {
                url = try RSDFileResultUtility.createFileURL(identifier: self.step.identifier, ext: "jpg", outputDirectory: outputDir, shouldDeletePrevious: true)
                self.save(jpegData, to: url!)
            }
        } catch let error {
            debugPrint("Failed to save the camera image: \(error)")
        }
        
        // Create the result and set it as the result for this step
        var result = RSDFileResultObject(identifier: self.step.identifier)
        result.url = url
        result.contentType = PSRImageHelper.contentTypeJpeg
        _ = self.stepViewModel.parent?.taskResult.appendStepHistory(with: result)
        
        // Go to the next step.
        self.goForward()
    }
    
    /// AVCaptureVideoDataOutputSampleBufferDelegate
    public func captureOutput(_ output: AVCaptureOutput,
                       didOutput sampleBuffer: CMSampleBuffer,
                       from connection: AVCaptureConnection) {
        
        // Vision framework is not available on simulator, needs > A7 chip
        #if TARGET_OS_SIMULATOR
            return
        #endif
        
        guard #available(iOS 14.0, *) else {
            return
        }
        
        let handler = VNImageRequestHandler(cmSampleBuffer: sampleBuffer,
                                            orientation: .up, options: [:])
        do {
            try handler.perform([self.handPoseRequest])
            guard
                let results = self.handPoseRequest.results?.prefix(2),
              !results.isEmpty
            else {
              return
            }            
            
            var handPose: HandPose? = nil
            results.forEach { observation in
                if let newHand = HandPose.create(observation: observation) {
                    handPose = newHand
                }
            }
            
            DispatchQueue.main.async {
                guard let handPoseUnwrapped = handPose else {
                    return
                }
                
                self.clearHand()
                handPoseUnwrapped.createHand(videoPreviewLayer: self.previewView.videoPreviewLayer).forEach({
                    self.handOverlayView.layer.addSublayer($0)
                })
                
                // Cancel previous attempts
                NSObject.cancelPreviousPerformRequests(withTarget: self, selector: #selector(self.hideHand), object: nil)
                
                // Set up a delayed call to hide the hand if we lose tracking after a second
                self.perform(#selector(self.hideHand), with: nil, afterDelay: 1)
            }
        } catch {
            print("Cannot perform hand pose tracking")
        }
    }
    
    public func clearHand() {
        // Clear all shape layers
        self.handOverlayView.layer.sublayers?.filter({
            $0.isKind(of: CAShapeLayer.self)
        }).forEach({
            $0.removeFromSuperlayer()
        })
    }
    
    @objc public func hideHand() {
        self.clearHand()
    }
}

public enum CameraDeviceWrapper: String, Codable, CaseIterable  {
    case front, rear, both
    
    func cameraDevice() -> UIImagePickerController.CameraDevice? {
        if self == .front {
            return UIImagePickerController.CameraDevice.front
        } else if self == .rear {
            return UIImagePickerController.CameraDevice.rear
        } else {
            return nil
        }
    }
}

class CameraPreviewView: UIView {
    var videoPreviewLayer: AVCaptureVideoPreviewLayer {
        guard let layer = layer as? AVCaptureVideoPreviewLayer else {
            fatalError("Expected `AVCaptureVideoPreviewLayer` type for layer. Check PreviewView.layerClass implementation.")
        }
        return layer
    }
    
    var session: AVCaptureSession? {
        get {
            return videoPreviewLayer.session
        }
        set {
            videoPreviewLayer.session = newValue
        }
    }
    
    override class var layerClass: AnyClass {
        return AVCaptureVideoPreviewLayer.self
    }
}

class PhotoCaptureProcessor: NSObject {
    private(set) var requestedPhotoSettings: AVCapturePhotoSettings
    
    private let willCapturePhotoAnimation: () -> Void
    
    lazy var context = CIContext()
    
    private let stopSessionRequestHandler: (PhotoCaptureProcessor) -> Void
    private let completionHandler: (PhotoCaptureProcessor) -> Void
    
    private let photoProcessingHandler: (Bool) -> Void
    
    /// The photo captured, will be non-nil after capture completes
    public var photoData: Data?
    
    /// When non-nil, this will be used to crop captured photo to video preview size
    public var metaDataOutputRect: CGRect?
    
    private var maxPhotoProcessingTime: CMTime?
    
    init(with requestedPhotoSettings: AVCapturePhotoSettings,
         willCapturePhotoAnimation: @escaping () -> Void,
         completionHandler: @escaping (PhotoCaptureProcessor) -> Void,
         stopSessionRequestHandler: @escaping (PhotoCaptureProcessor) -> Void,
         photoProcessingHandler: @escaping (Bool) -> Void) {
        
        self.requestedPhotoSettings = requestedPhotoSettings
        self.willCapturePhotoAnimation = willCapturePhotoAnimation
        self.completionHandler = completionHandler
        self.stopSessionRequestHandler = stopSessionRequestHandler
        self.photoProcessingHandler = photoProcessingHandler
    }
    
    private func didFinish() {
        completionHandler(self)
    }    
}

extension PhotoCaptureProcessor: AVCapturePhotoCaptureDelegate {
    
    /*
     This extension adopts all of the AVCapturePhotoCaptureDelegate protocol methods.
     */
    
    /// - Tag: WillBeginCapture
    func photoOutput(_ output: AVCapturePhotoOutput, willBeginCaptureFor resolvedSettings: AVCaptureResolvedPhotoSettings) {
        NSLog("WillBeginCapture")
        // No-op
    }
    
    /// - Tag: WillCapturePhoto
    func photoOutput(_ output: AVCapturePhotoOutput, willCapturePhotoFor resolvedSettings: AVCaptureResolvedPhotoSettings) {
        
        NSLog("WillCapturePhoto")
        
        willCapturePhotoAnimation()
        
        guard let maxPhotoProcessingTime = maxPhotoProcessingTime else {
            return
        }
        
        // Show a spinner if processing time exceeds one second.
        let oneSecond = CMTime(seconds: 1, preferredTimescale: 1)
        if maxPhotoProcessingTime > oneSecond {
            photoProcessingHandler(true)
        }
    }
    
    /// - Tag: DidFinishProcessingPhoto
    func photoOutput(_ output: AVCapturePhotoOutput, didFinishProcessingPhoto photo: AVCapturePhoto, error: Error?) {
        
        NSLog("DidFinishProcessingPhoto")
        
        photoProcessingHandler(false)
        
        if let error = error {
            print("Error capturing photo: \(error)")
        }
        
        // Check if there is any error in capturing
        guard error == nil else {
            print("Fail to capture photo: \(String(describing: error))")
            return
        }

        // Check if the pixel buffer could be converted to image data
        guard let imageData = photo.fileDataRepresentation() else {
            print("Fail to convert pixel buffer")
            return
        }

        // Check if UIImage could be initialized with image data
        guard let capturedImage = UIImage.init(data: imageData , scale: 1.0) else {
            print("Fail to convert image data to UIImage")
            return
        }
        
        stopSessionRequestHandler(self)
        
        // Allow for both full size image and
        // cropped to video preview if metaDataOutputRect is set
        var outputRect = CGRect(x: 0, y: 0, width: 1, height: 1)
        if let metaOutput = self.metaDataOutputRect {
            outputRect = metaOutput
        }

        var cgImage = capturedImage.cgImage!
        let width = CGFloat(cgImage.width)
        let height = CGFloat(cgImage.height)
        let cropRect = CGRect(x: outputRect.origin.x * width, y: outputRect.origin.y * height, width: outputRect.size.width * width, height: outputRect.size.height * height)

        cgImage = cgImage.cropping(to: cropRect)!
        let imageToSave = UIImage(cgImage: cgImage, scale: 1.0, orientation: .right)
        
        self.photoData = imageToSave.fixOrientationForPNG().pngData()
        
        self.didFinish()
    }        
    
    /// - Tag: DidFinishRecordingLive
    func photoOutput(_ output: AVCapturePhotoOutput, didFinishRecordingLivePhotoMovieForEventualFileAt outputFileURL: URL, resolvedSettings: AVCaptureResolvedPhotoSettings) {
        // No-op recording live
        NSLog("DidFinishRecordingLive")
    }
    
    /// - Tag: DidFinishProcessingLive
    func photoOutput(_ output: AVCapturePhotoOutput, didFinishProcessingLivePhotoToMovieFileAt outputFileURL: URL, duration: CMTime, photoDisplayTime: CMTime, resolvedSettings: AVCaptureResolvedPhotoSettings, error: Error?) {
        
        NSLog("photoOutput")
        
        if error != nil {
            print("Error processing Live Photo companion movie: \(String(describing: error))")
            return
        }
    }
    
    /// - Tag: DidFinishCapture
    func photoOutput(_ output: AVCapturePhotoOutput, didFinishCaptureFor resolvedSettings: AVCaptureResolvedPhotoSettings, error: Error?) {
        
        NSLog("DidFinishCapture")
        
        if let error = error {
            print("Error capturing photo: \(error)")
            didFinish()
            return
        }
        
        guard let _ = photoData else {
            print("No photo data resource")
            didFinish()
            return
        }
    }
}
fileprivate protocol PhotoCaptureCompleteDelegate {
    func photoCaptureComplete(pngData: Data)
}
